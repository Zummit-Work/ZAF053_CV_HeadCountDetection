{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205a3e25-a95c-4b24-94d9-584f0a3e96ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3988,
     "status": "ok",
     "timestamp": 1671550660273,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "_E7dKHHnqE6G",
    "outputId": "3690bbed-bc59-49a1-9dd6-6ae7f5468bb0"
   },
   "source": [
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476befa-5bff-4150-bb2a-2cb20d1c59dd",
   "metadata": {
    "id": "d476befa-5bff-4150-bb2a-2cb20d1c59dd"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d849f3f0-9d9a-4a13-9aee-78448fa0ed27",
   "metadata": {
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1671550660951,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "d849f3f0-9d9a-4a13-9aee-78448fa0ed27",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "class ImageDataLoader():\n",
    "    def __init__(self, data_path, gt_path, shuffle=False, gt_downsample=False, pre_load=False, num_classes=10):\n",
    "        #pre_load: if true, all training and validation images are loaded into CPU RAM for faster processing.\n",
    "        #          This avoids frequent file reads. Use this only for small datasets.\n",
    "        #num_classes: total number of classes into which the crowd count is divided (default: 10 as used in the paper)\n",
    "        self.data_path = data_path\n",
    "        self.gt_path = gt_path\n",
    "        self.gt_downsample = gt_downsample\n",
    "        self.pre_load = pre_load\n",
    "        self.data_files = [filename for filename in os.listdir(data_path) \\\n",
    "                           if os.path.isfile(os.path.join(data_path,filename))]\n",
    "        self.data_files.sort()\n",
    "        self.shuffle = shuffle\n",
    "        if shuffle:\n",
    "            random.seed(2468)\n",
    "        self.num_samples = len(self.data_files)\n",
    "        self.blob_list = {}        \n",
    "        self.id_list = range(0,self.num_samples)\n",
    "        self.min_gt_count = sys.maxsize\n",
    "        self.max_gt_count = 0\n",
    "        self.num_classes = num_classes\n",
    "        self.count_class_hist = np.zeros(self.num_classes)        \n",
    "        if self.pre_load:\n",
    "            self.preload_data() #load input images and grount truth into memory                \n",
    "            self.assign_gt_class_labels() #assign ground truth crowd group/class labels to each image\n",
    "            \n",
    "        else:\n",
    "            self.get_stats_in_dataset() #get min - max crowd count present in the dataset. used later for assigning crowd group/class\n",
    "            \n",
    "    \n",
    "    def get_classifier_weights(self):\n",
    "        #since the dataset is imbalanced, classifier weights are used to ensure balance.\n",
    "        #this function returns weights for each class based on the number of samples available for each class\n",
    "        wts = self.count_class_hist\n",
    "        wts = 1-wts/(sum(wts));\n",
    "        wts = wts/sum(wts);\n",
    "        return wts\n",
    "        \n",
    "    def preload_data(self):\n",
    "        print( 'Pre-loading the data. This may take a while...' )\n",
    "        idx = 0\n",
    "        for fname in self.data_files:            \n",
    "            img, den, gt_count = self.read_image_and_gt(fname)\n",
    "            self.min_gt_count = min(self.min_gt_count, gt_count)\n",
    "            self.max_gt_count = max(self.max_gt_count, gt_count)\n",
    "            \n",
    "            blob = {}\n",
    "            blob['data']=img\n",
    "            blob['gt_density']=den\n",
    "            blob['fname'] = fname                                \n",
    "            blob['gt_count'] = gt_count\n",
    "            \n",
    "            self.blob_list[idx] = blob\n",
    "            idx = idx+1\n",
    "            if idx % 100 == 0:                               \n",
    "                print( 'Loaded ', idx , '/' , self.num_samples )\n",
    "        print( 'Completed laoding ' ,idx, 'files' )\n",
    "        \n",
    "        \n",
    "    def assign_gt_class_labels(self):        \n",
    "        for i in range(0,self.num_samples):\n",
    "            gt_class_label = np.zeros(self.num_classes, dtype=np.int32)\n",
    "            bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
    "            class_idx = np.round(self.blob_list[i]['gt_count']/bin_val)\n",
    "            class_idx = int(min(class_idx,self.num_classes-1))\n",
    "            gt_class_label[class_idx]=1\n",
    "            self.blob_list[i]['gt_class_label'] = gt_class_label.reshape(1,gt_class_label.shape[0])\n",
    "            self.count_class_hist[class_idx] += 1\n",
    "            \n",
    "                    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:            \n",
    "            if self.pre_load:            \n",
    "                random.shuffle(self.id_list)        \n",
    "            else:\n",
    "                random.shuffle(self.data_files)\n",
    "                \n",
    "        files = self.data_files\n",
    "        id_list = self.id_list\n",
    "       \n",
    "        \n",
    "        for idx in id_list:\n",
    "            if self.pre_load:\n",
    "                blob = self.blob_list[idx]    \n",
    "                blob['idx'] = idx\n",
    "            else:\n",
    "                                    \n",
    "                fname = files[idx]\n",
    "                img, den, gt_count = self.read_image_and_gt(fname)\n",
    "                gt_class_label = np.zeros(self.num_classes,dtype=np.int32)\n",
    "                bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
    "                class_idx = np.round(gt_count/bin_val)\n",
    "                class_idx = int(min(class_idx,self.num_classes-1) )             \n",
    "                gt_class_label[class_idx] = 1\n",
    "                \n",
    "                blob = {}\n",
    "                blob['data']=img\n",
    "                blob['gt_density']=den\n",
    "                blob['fname'] = fname\n",
    "                blob['gt_count'] = gt_count\n",
    "                blob['gt_class_label'] = gt_class_label.reshape(1,gt_class_label.shape[0])\n",
    "                \n",
    "                \n",
    "                \n",
    "            yield blob\n",
    "    \n",
    "    def get_stats_in_dataset(self):\n",
    "        \n",
    "        min_count = sys.maxint\n",
    "        max_count = 0\n",
    "        gt_count_array = np.zeros(self.num_samples)\n",
    "        i = 0\n",
    "        for fname in self.data_files:\n",
    "            den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix()                        \n",
    "            den  = den.astype(np.float32, copy=False)\n",
    "            gt_count = np.sum(den)\n",
    "            min_count = min(min_count, gt_count)\n",
    "            max_count = max(max_count, gt_count)\n",
    "            gt_count_array[i] = gt_count\n",
    "            i+=1\n",
    "        \n",
    "        self.min_gt_count = min_count\n",
    "        self.max_gt_count = max_count        \n",
    "        bin_val = (self.max_gt_count - self.min_gt_count)/float(self.num_classes)\n",
    "        class_idx_array = np.round(gt_count_array/bin_val)\n",
    "        \n",
    "        for class_idx in class_idx_array:\n",
    "            class_idx = int(min(class_idx, self.num_classes-1))\n",
    "            self.count_class_hist[class_idx]+=1\n",
    "            \n",
    "        \n",
    "    def get_num_samples(self):\n",
    "        return self.num_samples\n",
    "                \n",
    "    def read_image_and_gt(self,fname):\n",
    "        \n",
    "        from scipy.io import loadmat\n",
    "        \n",
    "        img = cv2.imread(os.path.join(self.data_path,fname),0)\n",
    "        img = img.astype(np.float32, copy=False)\n",
    "        ht = img.shape[0]\n",
    "        wd = img.shape[1]\n",
    "        ht_1 = ht\n",
    "        wd_1 = wd\n",
    "        img = cv2.resize(img,(wd_1,ht_1) )\n",
    "        img = img.reshape((1,1,img.shape[0],img.shape[1]))\n",
    "        \n",
    "        #den = pd.read_csv(os.path.join(self.gt_path,os.path.splitext(fname)[0] + '.csv'), sep=',',header=None).as_matrix() \n",
    "        \n",
    "        ##########################\n",
    "        matfile = loadmat( os.path.join( self.gt_path, os.path.splitext(fname)[0] + '.mat' ) )  \n",
    "        \n",
    "        #print(matfile)\n",
    "        #print( matfile.get('image_info')[0][0][0][0][0] )\n",
    "        \n",
    "        den = np.array( matfile.get('image_info')[0][0][0][0][0] )\n",
    "        \n",
    "        #print(den)\n",
    "        \n",
    "        den  = den.astype(np.float32, copy=False)\n",
    "        if self.gt_downsample:\n",
    "            wd_1 = int(wd_1/4)\n",
    "            ht_1 = int(ht_1/4)\n",
    "            den = cv2.resize(den,(wd_1,ht_1))                \n",
    "            den = den * ((wd*ht)/(wd_1*ht_1))\n",
    "        else:\n",
    "            den = cv2.resize(den,(wd_1,ht_1))\n",
    "            den = den * ((wd*ht)/(wd_1*ht_1))\n",
    "            \n",
    "        den = den.reshape((1,1,den.shape[0],den.shape[1]))  \n",
    "        gt_count = np.sum(den)\n",
    "        \n",
    "        return img, den, gt_count\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cf1f-ae35-4ef9-8eeb-2f414e5cd0a7",
   "metadata": {
    "id": "06d2cf1f-ae35-4ef9-8eeb-2f414e5cd0a7"
   },
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ced606c-6ce1-4741-bc3b-f4385a7897cb",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1671550660952,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "9ced606c-6ce1-4741-bc3b-f4385a7897cb",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from crowd_count import CrowdCounter\n",
    "# import network\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(trained_model, data_loader):    \n",
    "    net = CrowdCounter()\n",
    "    #network.\n",
    "    load_net(trained_model, net)\n",
    "    net.cuda()\n",
    "    net.eval()\n",
    "    mae = 0.0\n",
    "    mse = 0.0\n",
    "    for blob in data_loader:                        \n",
    "        im_data = blob['data']\n",
    "        gt_data = blob['gt_density']\n",
    "        density_map = net(im_data, gt_data)\n",
    "        density_map = density_map.data.cpu().numpy()\n",
    "        gt_count = np.sum(gt_data)\n",
    "        et_count = np.sum(density_map)\n",
    "        mae += abs(gt_count-et_count)\n",
    "        mse += ((gt_count-et_count)*(gt_count-et_count))        \n",
    "    mae = mae/data_loader.get_num_samples()\n",
    "    mse = np.sqrt(mse/data_loader.get_num_samples())\n",
    "    return mae,mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91384a08-b650-47f1-88ec-09ba342f0233",
   "metadata": {
    "id": "91384a08-b650-47f1-88ec-09ba342f0233"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9254eaa9-6547-41c3-a40f-3d103782abd1",
   "metadata": {
    "executionInfo": {
     "elapsed": 2638,
     "status": "ok",
     "timestamp": 1671550663584,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "9254eaa9-6547-41c3-a40f-3d103782abd1",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from network import Conv2d, FC\n",
    "\n",
    "\n",
    "class CMTL(nn.Module):\n",
    "    '''\n",
    "    Implementation of CNN-based Cascaded Multi-task Learning of High-level Prior and Density\n",
    "    Estimation for Crowd Counting (Sindagi et al.)\n",
    "    '''\n",
    "    def __init__(self, bn=False, num_classes=10):\n",
    "        super(CMTL, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.base_layer = nn.Sequential(Conv2d( 1, 16, 9, same_padding=True, NL='prelu', bn=bn),                                     \n",
    "                                        Conv2d(16, 32, 7, same_padding=True, NL='prelu', bn=bn))\n",
    "        \n",
    "        self.hl_prior_1 = nn.Sequential(Conv2d( 32, 16, 9, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(16, 32, 7, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(32, 16, 7, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     Conv2d(16, 8,  7, same_padding=True, NL='prelu', bn=bn))\n",
    "                \n",
    "        self.hl_prior_2 = nn.Sequential(nn.AdaptiveMaxPool2d((32,32)),\n",
    "                                        Conv2d( 8, 4, 1, same_padding=True, NL='prelu', bn=bn))\n",
    "        \n",
    "        self.hl_prior_fc1 = FC(4*1024,512, NL='prelu')\n",
    "        self.hl_prior_fc2 = FC(512,256,    NL='prelu')\n",
    "        self.hl_prior_fc3 = FC(256, self.num_classes,     NL='prelu')\n",
    "        \n",
    "        \n",
    "        self.de_stage_1 = nn.Sequential(Conv2d( 32, 20, 7, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(20, 40, 5, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     nn.MaxPool2d(2),\n",
    "                                     Conv2d(40, 20, 5, same_padding=True, NL='prelu', bn=bn),\n",
    "                                     Conv2d(20, 10, 5, same_padding=True, NL='prelu', bn=bn))\n",
    "        \n",
    "        self.de_stage_2 = nn.Sequential(Conv2d( 18, 24, 3, same_padding=True, NL='prelu', bn=bn),\n",
    "                                        Conv2d( 24, 32, 3, same_padding=True, NL='prelu', bn=bn),                                        \n",
    "                                        nn.ConvTranspose2d(32,16,4,stride=2,padding=1,output_padding=0,bias=True),\n",
    "                                        nn.PReLU(),\n",
    "                                        nn.ConvTranspose2d(16,8,4,stride=2,padding=1,output_padding=0,bias=True),\n",
    "                                        nn.PReLU(),\n",
    "                                        Conv2d(8, 1, 1, same_padding=True, NL='relu', bn=bn))\n",
    "        \n",
    "    def forward(self, im_data):\n",
    "        x_base = self.base_layer(im_data)\n",
    "        x_hlp1 = self.hl_prior_1(x_base)\n",
    "        x_hlp2 = self.hl_prior_2(x_hlp1)\n",
    "        x_hlp2 = x_hlp2.view(x_hlp2.size()[0], -1) \n",
    "        x_hlp = self.hl_prior_fc1(x_hlp2)\n",
    "        x_hlp = F.dropout(x_hlp, training=self.training)\n",
    "        x_hlp = self.hl_prior_fc2(x_hlp)\n",
    "        x_hlp = F.dropout(x_hlp, training=self.training)\n",
    "        x_cls = self.hl_prior_fc3(x_hlp)        \n",
    "        x_den = self.de_stage_1(x_base)        \n",
    "        x_den = torch.cat((x_hlp1,x_den),1)\n",
    "        x_den = self.de_stage_2(x_den)\n",
    "        return x_den, x_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3105a8-7938-48eb-838f-d20fed8bf85a",
   "metadata": {
    "id": "9c3105a8-7938-48eb-838f-d20fed8bf85a"
   },
   "source": [
    "### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf18dd6-beec-495c-a723-597c375f2736",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1671550663585,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "3cf18dd6-beec-495c-a723-597c375f2736",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "        \n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, NL='relu', same_padding=False, bn=False):\n",
    "        super(Conv2d, self).__init__()\n",
    "        padding = int((kernel_size - 1) / 2) if same_padding else 0\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0, affine=True) if bn else None\n",
    "        if NL == 'relu' :\n",
    "            self.relu = nn.ReLU(inplace=True) \n",
    "        elif NL == 'prelu':\n",
    "            self.relu = nn.PReLU() \n",
    "        else:\n",
    "            self.relu = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, in_features, out_features, NL='relu'):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        if NL == 'relu' :\n",
    "            self.relu = nn.ReLU(inplace=True) \n",
    "        elif NL == 'prelu':\n",
    "            self.relu = nn.PReLU() \n",
    "        else:\n",
    "            self.relu = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_net(fname, net):\n",
    "    import h5py\n",
    "    h5f = h5py.File(fname, mode='w')\n",
    "    for k, v in net.state_dict().items():\n",
    "        h5f.create_dataset(k, data=v.cpu().numpy())\n",
    "\n",
    "\n",
    "def load_net(fname, net):\n",
    "    import h5py\n",
    "    h5f = h5py.File(fname, mode='r')\n",
    "    for k, v in net.state_dict().items():        \n",
    "        param = torch.from_numpy(np.asarray(h5f[k])) \n",
    "        v.copy_(param)\n",
    "\n",
    "\n",
    "\n",
    "def np_to_variable(x, is_cuda=True, is_training=False, dtype=torch.FloatTensor):\n",
    "    if is_training:\n",
    "        v = Variable(torch.from_numpy(x).type(dtype))\n",
    "    else:\n",
    "        v = Variable(torch.from_numpy(x).type(dtype), requires_grad = False, volatile = True)\n",
    "    if is_cuda:\n",
    "        v = v.cuda()\n",
    "    return v\n",
    "\n",
    "\n",
    "def set_trainable(model, requires_grad):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "\n",
    "def weights_normal_init(model, dev=0.01):\n",
    "    if isinstance(model, list):\n",
    "        for m in model:\n",
    "            weights_normal_init(m, dev)\n",
    "    else:\n",
    "        for m in model.modules():            \n",
    "            if isinstance(m, nn.Conv2d):        \n",
    "                m.weight.data.normal_(0.0, dev)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0.0, dev)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebdf65-28f8-49b0-8e21-418aa0e1f7a5",
   "metadata": {
    "id": "cdebdf65-28f8-49b0-8e21-418aa0e1f7a5"
   },
   "source": [
    "### timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5538df38-f037-4cdc-b182-4124ee6c93f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1671550663586,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "5538df38-f037-4cdc-b182-4124ee6c93f4",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Timer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tot_time = 0.\n",
    "        self.calls = 0\n",
    "        self.start_time = 0.\n",
    "        self.diff = 0.\n",
    "        self.average_time = 0.\n",
    "\n",
    "    def tic(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def toc(self, average=True):\n",
    "        self.diff = time.time() - self.start_time\n",
    "        self.tot_time  += self.diff\n",
    "        self.calls += 1\n",
    "        self.average_time = self.tot_time / self.calls\n",
    "        if average:\n",
    "            return self.average_time\n",
    "        else:\n",
    "            return self.diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17be2a-7a40-430b-b048-834c00726c45",
   "metadata": {
    "id": "0e17be2a-7a40-430b-b048-834c00726c45"
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f13fb7-103c-4d9c-be28-b650cfe841e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1671550663587,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "04f13fb7-103c-4d9c-be28-b650cfe841e9",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_results(input_img, gt_data,density_map,output_dir, fname='results.png'):\n",
    "    input_img = input_img[0][0]\n",
    "    gt_data = 255*gt_data/np.max(gt_data)\n",
    "    density_map = 255*density_map/np.max(density_map)\n",
    "    gt_data = gt_data[0][0]\n",
    "    density_map= density_map[0][0]\n",
    "    if density_map.shape[1] != input_img.shape[1]:\n",
    "        density_map = cv2.resize(density_map, (input_img.shape[1],input_img.shape[0]))\n",
    "        gt_data = cv2.resize(gt_data, (input_img.shape[1],input_img.shape[0]))\n",
    "    result_img = np.hstack((input_img,gt_data,density_map))\n",
    "    cv2.imwrite(os.path.join(output_dir,fname),result_img)\n",
    "    \n",
    "\n",
    "def save_density_map(density_map,output_dir, fname='results.png'):    \n",
    "    density_map = 255*density_map/np.max(density_map)\n",
    "    density_map= density_map[0][0]\n",
    "    cv2.imwrite(os.path.join(output_dir,fname),density_map)\n",
    "    \n",
    "def display_results(input_img, gt_data,density_map):\n",
    "    input_img = input_img[0][0]\n",
    "    gt_data = 255*gt_data/np.max(gt_data)\n",
    "    density_map = 255*density_map/np.max(density_map)\n",
    "    gt_data = gt_data[0][0]\n",
    "    density_map= density_map[0][0]\n",
    "    if density_map.shape[1] != input_img.shape[1]:\n",
    "         input_img = cv2.resize(input_img, (density_map.shape[1],density_map.shape[0]))\n",
    "    result_img = np.hstack((input_img,gt_data,density_map))\n",
    "    result_img  = result_img.astype(np.uint8, copy=False)\n",
    "    cv2.imshow('Result', result_img)\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1a36f-eb86-4950-b025-a8afcdc976f6",
   "metadata": {
    "id": "b2f1a36f-eb86-4950-b025-a8afcdc976f6"
   },
   "source": [
    "### crowdcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7943bf33-dca7-4c57-b585-a8fa6e929c65",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1671550663591,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "7943bf33-dca7-4c57-b585-a8fa6e929c65",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0,'D:/python/PROJECTS/crowdcount-cascaded-mtl/src')\n",
    "\n",
    "#import network\n",
    "#from models import CMTL\n",
    "\n",
    "class CrowdCounter(nn.Module):\n",
    "    def __init__(self, ce_weights=None):\n",
    "        super(CrowdCounter, self).__init__()        \n",
    "        self.CCN = CMTL()\n",
    "        if ce_weights is not None:\n",
    "            ce_weights = torch.Tensor(ce_weights)\n",
    "            ce_weights = ce_weights.cuda()\n",
    "        self.loss_mse_fn = nn.MSELoss()\n",
    "        self.loss_bce_fn = nn.BCELoss(weight=ce_weights)\n",
    "        \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self.loss_mse + 0.0001*self.cross_entropy\n",
    "    \n",
    "    def forward(self,  im_data, gt_data=None, gt_cls_label=None, ce_weights=None):        \n",
    "        im_data = np_to_variable(im_data, is_cuda=True, is_training=self.training)        #network.                \n",
    "        density_map, density_cls_score = self.CCN(im_data)\n",
    "        density_cls_prob = F.softmax(density_cls_score)\n",
    "        \n",
    "        if self.training:                        \n",
    "            gt_data = network.np_to_variable(gt_data, is_cuda=True, is_training=self.training)            \n",
    "            gt_cls_label = network.np_to_variable(gt_cls_label, is_cuda=True, is_training=self.training,dtype=torch.FloatTensor)                        \n",
    "            self.loss_mse, self.cross_entropy = self.build_loss(density_map, density_cls_prob, gt_data, gt_cls_label, ce_weights)\n",
    "            \n",
    "            \n",
    "        return density_map\n",
    "    \n",
    "    def build_loss(self, density_map, density_cls_score, gt_data, gt_cls_label, ce_weights):\n",
    "        loss_mse = self.loss_mse_fn(density_map, gt_data)        \n",
    "        ce_weights = torch.Tensor(ce_weights)\n",
    "        ce_weights = ce_weights.cuda()\n",
    "        cross_entropy = self.loss_bce_fn(density_cls_score, gt_cls_label)\n",
    "        return loss_mse, cross_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pVchU3GVtqNA",
   "metadata": {
    "id": "pVchU3GVtqNA"
   },
   "source": [
    "# **Main** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3698a85b-43a7-4e9b-80fe-b92583a6aa44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14722,
     "status": "ok",
     "timestamp": 1671550678300,
     "user": {
      "displayName": "Afzal Qureshi",
      "userId": "07647317251768315234"
     },
     "user_tz": -330
    },
    "id": "3698a85b-43a7-4e9b-80fe-b92583a6aa44",
    "outputId": "e5f84a45-b4b5-419d-9370-864175977c06",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading the data. This may take a while...\n",
      "Completed laoding  2 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afzal\\AppData\\Local\\Temp/ipykernel_6768/2757876705.py:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  v = Variable(torch.from_numpy(x).type(dtype), requires_grad = False, volatile = True)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "# from src.crowd_count import CrowdCounter\n",
    "# from src.data_loader import ImageDataLoader\n",
    "#from src import utils\n",
    "#from src import network\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0,'D:/python/PROJECTS/crowdcount-cascaded-mtl/src')\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "vis = True\n",
    "save_output = True\n",
    "\n",
    "#test data and model file path\n",
    "data_path =  'data/original/shanghaitech/part_A_final/test_data/images/'\n",
    "gt_path = 'data/original/shanghaitech/part_A_final/test_data/ground_truth/'\n",
    "model_path = 'final_models/cmtl_shtechA_204.h5'\n",
    "output_dir = 'output/'\n",
    "\n",
    "# colab_path = '/content/drive/MyDrive/Colab Notebooks/Zummit/crowdcount-cascaded-mtl/'\n",
    "# data_path =  colab_path + 'data/original/shanghaitech/part_A_final/test_data/images/'\n",
    "# gt_path = colab_path + 'data/original/shanghaitech/part_A_final/test_data/ground_truth/'\n",
    "# model_path = colab_path + 'final_models/cmtl_shtechA_204.h5'\n",
    "# output_dir = colab_path + 'output/'\n",
    "\n",
    "model_name = os.path.basename(model_path).split('.')[0]\n",
    "file_results = os.path.join(output_dir,'results_' + model_name + '_.txt')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "output_dir = os.path.join(output_dir, 'density_maps_' + model_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "#load test data\n",
    "data_loader = ImageDataLoader(data_path, gt_path, shuffle=False, gt_downsample=True, pre_load=True)\n",
    "\n",
    "net = CrowdCounter()\n",
    "      \n",
    "trained_model = os.path.join(model_path)\n",
    "#network.\n",
    "load_net(trained_model, net)\n",
    "net.cuda()\n",
    "net.eval()\n",
    "mae = 0.0\n",
    "mse = 0.0\n",
    "for blob in data_loader:                        \n",
    "    im_data = blob['data']\n",
    "    gt_data = blob['gt_density']\n",
    "    density_map = net(im_data, gt_data)\n",
    "    density_map = density_map.data.cpu().numpy()\n",
    "    gt_count = np.sum(gt_data)\n",
    "    et_count = np.sum(density_map)\n",
    "    mae += abs(gt_count-et_count)\n",
    "    mse += ((gt_count-et_count)*(gt_count-et_count))\n",
    "    if vis:\n",
    "        #utils.\n",
    "        display_results(im_data, gt_data, density_map)\n",
    "    if save_output:\n",
    "        #utils.\n",
    "        save_density_map(density_map, output_dir, 'output_' + blob['fname'].split('.')[0] + '.png')\n",
    "        \n",
    "        \n",
    "        \n",
    "mae = mae/data_loader.get_num_samples()\n",
    "mse = np.sqrt(mse/data_loader.get_num_samples())\n",
    "print( 'MAE: %0.2f, MSE: %0.2f' % (mae,mse) )\n",
    "\n",
    "f = open(file_results, 'w') \n",
    "f.write('MAE: %0.2f, MSE: %0.2f' % (mae,mse))\n",
    "f.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "co185v5DSMYo",
   "metadata": {
    "id": "co185v5DSMYo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d476befa-5bff-4150-bb2a-2cb20d1c59dd",
    "06d2cf1f-ae35-4ef9-8eeb-2f414e5cd0a7",
    "91384a08-b650-47f1-88ec-09ba342f0233",
    "9c3105a8-7938-48eb-838f-d20fed8bf85a",
    "cdebdf65-28f8-49b0-8e21-418aa0e1f7a5",
    "0e17be2a-7a40-430b-b048-834c00726c45",
    "b2f1a36f-eb86-4950-b025-a8afcdc976f6"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
